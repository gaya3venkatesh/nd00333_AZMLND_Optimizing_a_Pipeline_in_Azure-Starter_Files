# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contain data about a direct marketing campaign of a financial institution, based on phone calls. The aim of the campaign was to increase the number of subscribers to the bank term deposit. In this project, we seek to predict a binary variable "y" ,i.e., whether a client will subscribe to the product -marked with "yes" - or not - marked with "no".

Reference: https://archive.ics.uci.edu/ml/datasets/bank+marketing

The best performing model was a StackEnsemble Classifier generated by AutoML with the accuracy score of 0.9159332321699545. The model showed a slightly better performance as compared to that of the HytperDrive approach with the accuracy score of 0.9081436519979768.

## Scikit-learn Pipeline
The pipeline consists of data preparation, training and validation stages.

Data Preparation

A csv file containing the dataset was first converted to a TabularDataset type by using TabularDatasetFactory and then to a pandas dataframe. The data was then cleaned and prepared for the next stage of the pipeline. The output of the stage are two dataframes - the predictor variables and the target variable.

Classifier

Logistic Regression from the Scikit-Learn library was used to demonstrate the HyperDrive approach.

Training configuration using HyperDrive Package

Hyperparameters to optimise:

C - regularisation strength
max_iter - maximum number of iterations required for the classifier to converge
Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression

The parameter search space:

'C': 0.0001, 0.001, 0.01, 0.1, 1,10,100,1000
'max_iter': 100, 200, 300, 400, 500
Sampling method: RandomParameterSampling - random search strategy to find the values

Primary metric to optimise: Accuracy

Early termination policy: Bandit Policy

Primary metric goal: PrimaryMetricGoal.MAXIMIZE

Max total runs: 15

Training and Validation

The data was split into train (70%) and test (30%) datasets. We optimised hyperparameters by fitting multiple models with different hyperparameters on the train set and validating the models using the test set. The best run was selected and saved.

**What are the benefits of the parameter sampler you chose?**
RandomParameterSampling is optimised for speed because it picks randomly hyperparameter values instead of going though every single value, i.e., it allows us to achieve an optimal primary metric for a relatively short period of time.

**What are the benefits of the early stopping policy you chose?**

Bandit Policy terminates runs which fall outside of the top n% range every k interval, saving time of the experiment.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
