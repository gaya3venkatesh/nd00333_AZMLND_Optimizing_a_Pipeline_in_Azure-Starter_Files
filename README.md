# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contain data about a direct marketing campaign of a financial institution, based on phone calls. The aim of the campaign was to increase the number of subscribers to the bank term deposit. In this project, we seek to predict a binary variable "y" ,i.e., whether a client will subscribe to the product -marked with "yes" - or not - marked with "no".

Reference: https://archive.ics.uci.edu/ml/datasets/bank+marketing

The best performing model was a StackEnsemble Classifier generated by AutoML with the accuracy score of 0.9159332321699545. The model showed a slightly better performance as compared to that of the HytperDrive approach with the accuracy score of 0.9081436519979768.

## Scikit-learn Pipeline
The pipeline consists of data preparation, training and validation stages.

Data Preparation

A csv file containing the dataset was first converted to a TabularDataset type by using TabularDatasetFactory and then to a pandas dataframe. The data was then cleaned and prepared for the next stage of the pipeline. The output of the stage are two dataframes - the predictor variables and the target variable.

Classifier

Logistic Regression from the Scikit-Learn library was used to demonstrate the HyperDrive approach.

Training configuration using HyperDrive Package

Hyperparameters to optimise:

C - regularisation strength
max_iter - maximum number of iterations required for the classifier to converge
Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression

The parameter search space:

'C': 0.0001, 0.001, 0.01, 0.1, 1,10,100,1000
'max_iter': 100, 200, 300, 400, 500
Sampling method: RandomParameterSampling - random search strategy to find the values

Primary metric to optimise: Accuracy

Early termination policy: Bandit Policy

Primary metric goal: PrimaryMetricGoal.MAXIMIZE

Max total runs: 100

Training and Validation

The data was split into train (80%) and test (20%) datasets. We optimised hyperparameters by fitting multiple models with different hyperparameters on the train set and validating the models using the test set. The best run was selected and saved.

**What are the benefits of the parameter sampler you chose?**
RandomParameterSampling is optimised for speed because it picks randomly hyperparameter values instead of going though every single value, i.e., it allows us to achieve an optimal primary metric for a relatively short period of time.

**What are the benefits of the early stopping policy you chose?**

Bandit Policy terminates runs which fall outside of the top n% range every k interval, saving time of the experiment.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

The model generated by AutoML was StackEnsemble consisting of a base model - XGBoostClassifier - and a final (or meta) model - LogisticRegressionCV.

The hyperparameters generated by AutoML:



## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The Hyperdrive and AutoML approaches produced quite similar accuracy scores, 0.9081436519979768 and 0.9159332321699545 respectively. A slightly greater result of the latter approach can be explained by the fact that AutoML selects estimators, performs feature engineering and chooses hyperparameters, unlike the first approach that deals with hyperparameter tuning only. In addition, the AutoML approach generated an ensemble model -- these types of models combine multiple models that, together, produce a better predictive accuracy as compared to a single model.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

We will need to tackle the issue outlined below by introducing techniques for handling imbalanced data. This will reduce the bias towards one class and, therefore, produce a more reliable model capable of recognising both classes. Alternatively, we can use a metric that is more suitable for working with imbalanced data, for example, AUC and F1 score, to get more reliable predictions

AutoML run details:

TYPE: Class balancing detection

STATUS: ALERTED

DESCRIPTION: To decrease model bias, please cancel the current run and fix balancing problem. Learn more about imbalanced data: https://aka.ms/AutomatedMLImbalancedData

DETAILS: Imbalanced data can lead to a falsely perceived positive effect of a model's accuracy because the input data has bias towards one class.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
